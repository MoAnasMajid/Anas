# -*- coding: utf-8 -*-
"""YOLOv10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UtNaAxoIH5eqqJ80Wy-N2gdulJ5clVie
"""

!pip install opencv-python ultralytics

# Commented out IPython magic to ensure Python compatibility.
from ultralytics import YOLO
import os
print(os.getcwd())
# %cd /content/{HOME}/weights
# Load a pre-trained YOLOv10n model
model = YOLO("yolov10n.pt")

from google.colab.patches import cv2_imshow
import cv2

video_path = "/content/Input model.webm"
cap = cv2.VideoCapture(video_path)  #reads frame from video

# Get the video's width, height, and frames per second (fps)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

output_video_path = "/content/Output_annotated_video.webm"  # Output video path
fourcc = cv2.VideoWriter_fourcc(*"VP90")  # Codec for WebM format,  VP90 for WebM format
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

output_txt_path = "/content/main_annotations.txt"  # Anotations path and saved in text format
with open(output_txt_path, 'w') as f:     #file in write mode

    frame_number = 1  # Start frame numbering from 1, In MOT challenge format frame numbering should start from 0
    while cap.isOpened():
        # Read a frame from the video
        success, frame = cap.read()

        if success:
            # Run YOLOv8 tracking on the frame, persisting tracks between frames
            results = model.track(frame, persist=True)

            # Extract bounding box information and write to the text file
            seen_ids = set()
            for track in results[0].boxes.data:
                # Unpack the track information
                id = int(track[5])

                # Skip if this ID has already been seen in the current frame
                if id in seen_ids:
                    continue

                bb_left = float(track[0])
                bb_top = float(track[1])
                bb_width = float(track[2]) - bb_left
                bb_height = float(track[3]) - bb_top
                conf = float(track[4])

                # Add the ID to the set of seen IDs
                seen_ids.add(id)

                # Format the annotation string
                annotation = f"{frame_number}, {id}, {bb_left}, {bb_top}, {bb_width}, {bb_height}, {conf}, -1, -1, -1\n"

                # Write the annotation to the file
                f.write(annotation)

            # Increment the frame number
            frame_number += 1

            # Visualize the results on the frame (optional)
            annotated_frame = results[0].plot()
            out.write(annotated_frame)

        else:
            # Break the loop if the end of the video is reached
            break

# Release the video capture and writer objects
cap.release()
out.release()
cv2.destroyAllWindows()

# Provide the path to the saved video file and annotations file
print(f"Annotated video saved at: {output_video_path}")
print(f"Annotations saved at: {output_txt_path}")

